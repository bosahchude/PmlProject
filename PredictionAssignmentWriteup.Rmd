---
title: "Prediction Assignment Writeup"
author: "Bosah Chude"
date: "Sunday, November 23, 2014"
output: 
     html_document:
          keep_md: true
---
## Executive Summary

The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.

After building the model, I apply cross validation to determine the expected out of sample error and other relevant statistical qualifier. 

The data was gotten from here: http://groupware.les.inf.puc-rio.br/har

## Pre Processing

Downloading and saving data.

```{r downloadSection, cache=TRUE, eval=FALSE}
#Download Data
if (!file.exists("pml-training.csv")) {
     
     fileTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
     fileTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
     
     download.file(fileTrain, "pml-training.csv", method="curl")
     download.file(fileTest, "pml-testing.csv", method="curl")
}
```

Setting the seed to ensure *reproducubility* and loading required libraries.

```{r, message=FALSE}
set.seed(125)
library(randomForest)
library(caret)
```


## Data Processing

The steps in this section involve formatting the data and removing unnecessary columns that are filled with NAs.

All columns with more than 90% NAs are dropped from the training set

```{r}
#Import the training data
trainingData <- read.csv("pml-training.csv", na.strings=c("NA", "", "#DIV/0!", " "))
trainingData <- trainingData[,8:160]

#Drop all columns with more than 10% NAs
trainingData <- trainingData[,colSums(is.na(trainingData)) < (nrow(trainingData) * 0.9)]

#Further partition the data
inTrain <- createDataPartition(y = trainingData$classe, p = 0.7, list = FALSE)
verificationData <- trainingData[-inTrain,]
trainingData <- trainingData[inTrain,]

#Pull the testing data
testingData <- read.csv("pml-testing.csv", na.strings=c("NA", "", "#DIV/0!", " "))
testingData <- testingData[, 8:160]

#Columns that would be used
names(trainingData)
```

## Modelling

The model of choice is "Random Forest" this is due to its documented accuracy in classification problems like these.

```{r}
#Fit the model
modelFit <- randomForest(classe ~ ., data = trainingData, ntree = 100)

#Plot of error rate vs trees
plot(modelFit, main = "OOB estimate of  error rate")

#Model Summary
modelFit

```

The model has an estimated out of sample error of 0.54, this means it is very effective.

## Cross Validation

Using the model to predict on the test data set.
```{r}
#Predict new values
validationValues <- predict(modelFit, newdata = verificationData)

#Display detailed result summary
confusionMatrix(verificationData$classe, validationValues)
```

## Testing

Predicting on the supplied test set.

```{r}
predictedValues <- predict(modelFit, newdata = testingData)

#All twenty predicted values
predictedValues
```

## Summary

The model is very efficient with an approximate 99% accuracy level. 
